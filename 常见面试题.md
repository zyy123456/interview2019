1. CNN 模型所需的计算力（FLOPs）和参数（parameters）数量怎么计算
   
  * 对于一个卷积层，假设其大小为<img src="http://latex.codecogs.com/gif.latex?k*k*c*n">, k是kernel大小, c是input channel的数目, n为output channel的数目, 输出的feature map尺寸<img src="http://latex.codecogs.com/gif.latex?H' * W'">, 则该卷积层的
      * <img src="http://latex.codecogs.com/gif.latex?n\_params = n * (k * k * c + 1)">
      * <img src="http://latex.codecogs.com/gif.latex?n\_FLOPs =H'*W'*n*(k*k*c+1)">

2. 转置卷积(反卷积)
  * [资源](https://datascience.stackexchange.com/questions/6107/what-are-deconvolutional-layers)

3. Kmeans 算法

 ```python
import numpy as np

def kMeans(X, K, maxIters = 10, plot_progress = None):

    centroids = X[np.random.choice(np.arange(len(X)), K)]
    for i in range(maxIters):
        # Cluster Assignment step
        C = np.array([np.argmin([np.dot(x_i-y_k, x_i-y_k) for y_k in centroids]) for x_i in X])
        # Ensure we have K clusters, otherwise reset centroids and start over
        # If there are fewer than K clusters, outcome will be nan.
        if (len(np.unique(C)) < K):
            centroids = X[np.random.choice(np.arange(len(X)), K)]
        else:
            # Move centroids step 
            centroids = [X[C == k].mean(axis = 0) for k in range(K)]
        if plot_progress != None: plot_progress(X, C, np.array(centroids))
    return np.array(centroids) , C
```
4. inception相关
  * [资源](https://imlogm.github.io/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/inception/)
  * 结构
      * GoogLeNet（Inception-v1）：相比AlexNet和VGG，出现了多支路，引入了1×1卷积帮助减少网络计算量
    * Inception-v2：引入Batch Normalization(BN)；5×5卷积使用两个3×3卷积代替
    * Inception-v3：n×n卷积分割为1×n和n×1两个卷积
    * Inception-v4：进一步优化，引入ResNet的shortcut思想
* Inception的目标是针对分类任务追求最高的精度，以至于后面几代开始“炼丹”，模型过于精细；Xception的目标是设计出易迁移、计算量小、能适应不同任务，且精度较高的模型。

5. NMS的计算

```python
def py_cpu_nms(dets, thresh):
    """Pure Python NMS baseline."""
    x1 = dets[:, 0]
    y1 = dets[:, 1]
    x2 = dets[:, 2]
    y2 = dets[:, 3]
    scores = dets[:, 4]

    areas = (x2 - x1 + 1) * (y2 - y1 + 1)
    order = scores.argsort()[::-1]

    keep = []
    while order.size > 0:
        i = order[0]
        keep.append(i)
        xx1 = np.maximum(x1[i], x1[order[1:]])
        yy1 = np.maximum(y1[i], y1[order[1:]])
        xx2 = np.minimum(x2[i], x2[order[1:]])
        yy2 = np.minimum(y2[i], y2[order[1:]])

        w = np.maximum(0.0, xx2 - xx1 + 1)
        h = np.maximum(0.0, yy2 - yy1 + 1)
        inter = w * h
        ovr = inter / (areas[i] + areas[order[1:]] - inter)

        inds = np.where(ovr <= thresh)[0]
        order = order[inds + 1]

    return keep
```
```
#RT:RightTop
#LB:LeftBottom
def IOU(rectangle A, rectangleB):
    W = min(A.RT.x, B.RT.x) - max(A.LB.x, B.LB.x) 
    H = min(A.RT.y, B.RT.y) - max(A.LB.y, B.LB.y) 
    if W <= 0 or H <= 0:
        return 0;
    SA = (A.RT.x - A.LB.x) * (A.RT.y - A.LB.y) 
    SB = (B.RT.x - B.LB.x) * (B.RT.y - B.LB.y) 
    cross = W * H
    return cross/(SA + SB - cross)
```
 6. Miou
![Screen Shot 2019-08-16 at 8.20.08 PM.png](https://upload-images.jianshu.io/upload_images/10167864-5ac36ed82774fd5e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
* [from](https://medium.com/digitalbridge/how-we-use-image-semantic-segmentation-e85fac734caf)
7. Batchnorm和其他norm
* [Batchnorm原理](https://zhuanlan.zhihu.com/p/62935978)
* [详解深度学习中的Normalization，BN/LN/WN](https://zhuanlan.zhihu.com/p/33173246)
8. 排序
* [归并排序](https://www.cnblogs.com/piperck/p/6030122.html)
* 稳定排序
    * 直接插入/冒泡排序/归并/基数排序
 9. 可变形卷积
10. CTCloss
11. SVD分解
12. 皮尔森系数
    * [link](https://blog.csdn.net/AlexMerer/article/details/74908435)

13.AUC的计算
   * AUC跟准确度、F1等指标相比有什么好处？
14. 多线程多进程
15. 七层OSI模型分别介绍一下
![](https://upload-images.jianshu.io/upload_images/10167864-c9fae90304f05084.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
16. resnet/resnext/se-resnext的架构，并实现一个简单的block
    * [code](https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/senet.py)
![](https://upload-images.jianshu.io/upload_images/10167864-1a5ef57736a1ab3d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


17. 实现高斯核与卷积过程
    * [link](https://blog.csdn.net/nlite827109223/article/details/90697377)

18. Focal loss具体的操作, 两个参数的作用
    * gamma>0使得减少易分类样本的损失。使得更关注于困难的、错分的样本
    * 平衡因子alpha，用来平衡正负样本本身的比例不均
    * [link](https://www.cnblogs.com/king-lps/p/9497836.html)
19. c语言中结构体和联合体的[区别](https://blog.csdn.net/sjtudou/article/details/81074916)
20. 哈希表解决冲突的几种[方式](https://www.cnblogs.com/westlife-11358/p/10038878.html)
21. [优化器原理](https://zhuanlan.zhihu.com/p/32230623), adam 怎么优化而来
22. L1 和 L2正则项的[区别](https://www.cnblogs.com/lyr2015/p/8718104.html)
23. [LSTM怎么避免梯度弥散的](https://www.zhihu.com/question/34878706/answer/654501152)
